{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5ee2c3a-36e7-4d15-8655-708c0a354dae",
   "metadata": {},
   "source": [
    "# Starbucks Promotional Offers Portfolio Project\n",
    "**Main Objectives**\n",
    "- Explore how different customer demographics (age, income, gender) engage with offers and how this impacts their purchasing power.\n",
    "- Analyse the effectiveness of various offer types accross channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce08b64f-69b6-4a6a-9ad8-e6af67c19cca",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmysql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnector\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mast\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m literal_eval\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\seaborn\\__init__.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpalettes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrelational\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcategorical\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\seaborn\\relational.py:21\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     14\u001b[0m     adjust_legend_subtitles,\n\u001b[0;32m     15\u001b[0m     _default_color,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     _scatter_legend_artist,\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_compat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m groupby_apply_include_groups\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_statistics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EstimateAggregator, WeightedAggregator\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maxisgrid\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FacetGrid, _facet_docs\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_docstrings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocstringComponents, _core_docs\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\seaborn\\_statistics.py:32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gaussian_kde\n\u001b[0;32m     33\u001b[0m     _no_scipy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\stats\\__init__.py:606\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m.. _statsrefmanual:\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    601\u001b[0m \n\u001b[0;32m    602\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_warnings_errors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[0;32m    605\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[1;32m--> 606\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stats_py\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_variation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m variation\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:49\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mspecial\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linalg\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distributions\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _mstats_basic \u001b[38;5;28;01mas\u001b[39;00m mstats_basic\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stats_mstats_common\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (_find_repeats, linregress, theilslopes,\n\u001b[0;32m     52\u001b[0m                                    siegelslopes)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\stats\\distributions.py:8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Author:  Travis Oliphant  2002-2011 with contributions from\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#          SciPy Developers 2004-2011\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#       instead of `git blame -Lxxx,+x`.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_distn_infrastructure\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (rv_discrete, rv_continuous, rv_frozen)  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _continuous_distns\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _discrete_distns\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimize\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# for functions of continuous distributions (e.g. moments, entropy, cdf)\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m integrate\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# to approximate the pdf of a continuous distribution given its cdf\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_finite_differences\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _derivative\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\__init__.py:134\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(name):\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m submodules:\n\u001b[1;32m--> 134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscipy.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\integrate\\__init__.py:97\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_odepack_py\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_quadpack_py\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m---> 97\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ode\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bvp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m solve_bvp\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ivp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (solve_ivp, OdeSolution, DenseOutput,\n\u001b[0;32m    100\u001b[0m                    OdeSolver, RK23, RK45, DOP853, Radau, BDF, LSODA)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\integrate\\_ode.py:88\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m asarray, array, zeros, isscalar, real, imag, vstack\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _vode\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _dop\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _lsoda\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mysql.connector\n",
    "\n",
    "from ast import literal_eval\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd8742a-94c1-4224-9796-26d2e2ae9447",
   "metadata": {},
   "source": [
    "## Importing the Data\n",
    "I will use the pandas library to import the data into this workbook. I will then check both the head and tail of each table to ensure that they were loaded in correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058cbcc7-f6be-478f-88e4-bb1f98a0f62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into pd.DataFrame objects\n",
    "customers = pd.read_csv('customers.csv')\n",
    "offers = pd.read_csv('offers.csv')\n",
    "events = pd.read_csv('events.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c13be5-a1c7-4bdf-9dd6-c39eac6f797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the offers table loaded in correctly by checking the head\n",
    "offers.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a38693c-830d-4c74-b033-e15516818d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the offers table loaded in correctly by checking the tail\n",
    "offers.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e8dd86-42bb-4d70-85ab-d4ddeb23caa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the customers table loaded in correctly by checking the head\n",
    "customers.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccaf1f1-229d-45a7-88a9-27e5f3914ca1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make sure the customers table loaded in correctly by checking the tail\n",
    "customers.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c115550d-743c-4f52-9bb0-cb1753ae32c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the events table loaded in correctly by checking the head\n",
    "events.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914f98d9-eec1-491f-b1e4-fd099285de1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the events table loaded in correctly by checking the tail\n",
    "events.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3222b941-a93e-4a15-86be-235705568047",
   "metadata": {},
   "source": [
    "## Cleaning the Data\n",
    "I will now clean each table in the dataset one by one. To do this, I will first make sure that any individual column in the table that is in an \"unclean\" format is put into a cleaner format. I will then investigate the data types of each column. At the same time, I will investigate the bit-sizes and potentail outliers of the numeric columns. I will make any necessary changes to these attributes. I will then investigate missing data and decide on an case-by-case basis what to do with the missing data. I will then specifically check if there are inconsitent text or typos in the categorical columns and make necessary fixes. I will then check for duplicated rows and decide whether it is necessary to delete them.\n",
    "### Clean the `offers` Table\n",
    "The `offers` table has a column called `channels` that is in the form of lists. Since Pandas does not work overally well with lists as values, I will create dummy variables for the individual channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea73a7d-57de-4508-b9f8-a10356120f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn channels column into list type\n",
    "offers['channels'] = offers['channels'].apply(literal_eval)\n",
    "\n",
    "# Create channels dummies\n",
    "for channel in ['web', 'email', 'mobile', 'social']:\n",
    "    offers[channel] = offers['channels'].apply(lambda x: 1 if channel in x else 0).astype('int8')\n",
    "\n",
    "# Drop the original columns list\n",
    "offers = offers.drop('channels', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015563b1-93be-4f1e-9b9d-484552229560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data types, size, and memory usage of the offers table\n",
    "offers.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11215c60-0163-4561-aa4c-4af2a02e063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the summary statistics for the numeric columns to see if we can reduce bit-sizes and identify possible outliers\n",
    "offers.describe().round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f22cab-d3af-40ee-ab70-2ceb229278e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the bit-sizes of the numeric columns to save memory and optimise efficiency\n",
    "offers = offers.astype({\n",
    "    'difficulty': 'int8',\n",
    "    'reward': 'int8',\n",
    "    'duration': 'int8',\n",
    "    'offer_type': 'category'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ef01c5-6c84-4c9f-a9d8-ae840e0c7c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if the changes occured\n",
    "offers.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814de947-30dc-48f4-868d-527b003202cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any missing data\n",
    "offers.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf49970-d563-45f3-8534-03872269cc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any inconsistent text or typos, specifically in category columns\n",
    "offers['offer_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe6756a-bb10-40b8-a279-9dabbd21166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any duplicate rows\n",
    "offers.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0c5ab9-f87b-4ec2-939b-160770e79946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the cleaned version of the offers table\n",
    "offers.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de007972-43a9-44a8-8be9-64d904cb9d11",
   "metadata": {},
   "source": [
    "### Cleaning the `customers` Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d7e423-1c4f-461e-9832-349d18bf2b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data types, size, and memory usage of the customers data frame\n",
    "customers.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a91130a-5adf-45cd-916a-6b774ef41687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the summary statistics for the numeric columns to see if we can reduce bit-sizes and identify possible outliers\n",
    "customers.describe().round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f083cc-a814-4107-827b-755c3a0a7d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if gender is a categorical variable\n",
    "customers['gender'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf29c04-58e1-4260-95cb-f25ad6aaa17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data types of the column in the wrong data type and reduce bit-sizes of numeric columns to save space\n",
    "customers = customers.astype({\n",
    "    'gender': 'category',\n",
    "    'age': 'int16',\n",
    "    'income': 'float32'\n",
    "})\n",
    "\n",
    "# Convert the became_member_on column to datetime dtype\n",
    "customers['became_member_on'] = pd.to_datetime(customers['became_member_on'], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fff51a1-79ed-47da-b1d5-e18c6ba3cbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if the changes occured\n",
    "customers.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1a76f7-fdf6-4124-9710-2bc3c235c072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any missing data\n",
    "customers.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2654cd7-fc7f-4f09-a540-502d5078fe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the rows that have missing values\n",
    "customers_nan_rows = customers[customers.isna().any(axis=1)]\n",
    "customers_nan_rows.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640b4769-b01e-4ccf-aa42-ae8078668dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see how many rows have values missing\n",
    "len(customers_nan_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fd2dea-9808-4cc2-b7dc-7eb63b164d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the ages of all the missing values\n",
    "customers_nan_rows['age'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d823a2d-79f7-48a0-9b07-bdce1432e620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many 118 values there are in the age column\n",
    "(customers['age'] == 118).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26090ad5-22ea-45fb-9336-afb99c3ccd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the % of missing data\n",
    "(len(customers_nan_rows) / len(customers)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fa2eaa-dae2-45b9-83db-9779c1595911",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers['age'].hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67a2304-2230-4414-8ca8-aad9cde6476b",
   "metadata": {},
   "source": [
    "In `customers`, there are 2175 missing values each for both the `gender` and `income` columns. These missing values all occur in the same rows, meaning that there are 2175 rows that miss both the `gender` and `income` values. In addition, each one of these rows has `118` for its value for `age`, indicating that this is a placeholder for missing information. Moreover, it is skewing the distribution for the `age` column, so we will change these values to `np.NaN` values. Even though the rows with missing values make up 12.8% of our data, I will leave all the missing values as is for now so we do not lose any info when we join the tables later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e56c58-ac45-4de6-b7e4-bb812dc686a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the values of 118 in the age column with np.NaN values\n",
    "customers['age'] = customers['age'].replace({118: np.NaN})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c422c33-64dd-4a8c-8828-07ce272e7d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for dupilcate rows\n",
    "customers.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee2ba9f-bb4d-41b8-8620-53c9131bb793",
   "metadata": {},
   "source": [
    "### Cleaning the `events` Table\n",
    "The `events` table has a column called `value` that is in the form of dictionaries. Since Pandas does not work overally well with dictionaries as values, I will seperate the keys and values into seperate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1fbe44-2e2b-446c-b1f6-c677f7fe1dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn channels column into dict type\n",
    "events['value'] = events['value'].apply(literal_eval)\n",
    "\n",
    "# Create a series that holds the dictionary keys as a list\n",
    "value_keys = events['value'].apply(lambda dict: list(dict.keys()))\n",
    "\n",
    "# Create a series that holds the dictionary values as a list\n",
    "value_values = events['value'].apply(lambda dict: list(dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d7c628-b960-4320-8da3-17b87543868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see what values are in the dictionaries by checking the different possible keys.\n",
    "value_keys.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0890679e-10f3-481d-9821-248e8875f542",
   "metadata": {},
   "source": [
    "It looks like the `value` column has three different types of keys, which are `amount`, `offer_id`/`offer id`, and `reward`. Upon further investigation, the `value` values that have `amount` as their key are `transaction` events, the `value` values that just have `offer_id`/`offer id` as their key are `offer received` and `offer viewed` events, and `value` values that have both `offer_id` and `reward` as their keys are `offer completed` events. Hence, I will now create columns for these individual keys and corresponding values. However, for reward, I will just create a column called `reward` and fill it with the value or `NaN` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d2a071-ee64-43eb-9c2e-b2a2deb8383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column that hold the value key (either amount or offer id/offer_id)\n",
    "events['value_key'] = value_keys.apply(lambda lst: lst[0])\n",
    "\n",
    "# Create a column that holds the value value (either the amount or the offer id/offer_id)\n",
    "events['value'] = value_values.apply(lambda lst: lst[0])\n",
    "\n",
    "#Create a column that holds the reward value if it exists\n",
    "events['reward'] = value_values.apply(lambda lst: lst[1] if len(lst) == 2 else np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990c6adf-3b77-483e-9dbd-9d94c2b6c411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the time column into two columns that is the day number and hour of that day\n",
    "events['day'] = events['time'] // 24\n",
    "events['hour'] = events['time'] % 24\n",
    "\n",
    "# Drop the original time column\n",
    "events = events.drop('time', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38a4df4-81e4-4a02-8f36-01d3083648e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data types, size, and memory usage of the events table\n",
    "events.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4705ea-89f2-4ce5-9cb3-12e7605f1f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the summary statistics for the numeric columns to see if we can reduce bit-sizes and identify possible outliers\n",
    "events.describe().round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941b17dd-c810-4c22-a47e-e4ac702c9906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the events column is categorical and if there is any inconsistent text or typos \n",
    "events['event'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311572aa-ee73-4963-8301-5a62a205924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the events column is categorical and if there is any inconsistent text or typos \n",
    "events['value_key'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98c2337-b5cf-4c57-ba05-8eb57cd956b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the inconsitent text\n",
    "events['value_key'] = events['value_key'].replace({'offer_id': 'offer id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a9bb9d-2e21-4fdc-ba50-e40e92131de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if the inconsitent text was fixed\n",
    "events['value_key'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e49b41-6aa1-4856-9e85-eeefa516f9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data types of the column in the wrong data type and reduce bit-sizes of numeric columns to save space\n",
    "events = events.astype({\n",
    "    'event': 'category',\n",
    "    'value_key': 'category',\n",
    "    'reward': 'float32',\n",
    "    'day': 'int8',\n",
    "    'hour': 'int8'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1fc960-f708-4dda-b0ac-31eb8d78114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if the changes occured\n",
    "events.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461f4305-b4a2-4733-aeb7-97b430489986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "events.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be074136-3b45-44d0-8f12-5036b0287a4f",
   "metadata": {},
   "source": [
    "These missing values are a reflection of the amount of events in the table that are not `offer completed`. Therefore we want these values to remain missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7310802-aa63-40e8-a777-bf92d8d9b1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "events.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf82fce3-f411-4f6b-830e-26dc5a6d6578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the duplicate rows\n",
    "events_dup_rows = events[events.duplicated()]\n",
    "events_dup_rows.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f263a6f-77fd-4df8-850a-f5fa825fc584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the event types of the duplicate rows\n",
    "events_dup_rows['event'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464f26f0-9ada-42e1-94bd-ad510ace63e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the offer types of duplicate rows\n",
    "events_dup_rows.merge(offers, how='inner', left_on='value', right_on='offer_id')['offer_type'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2840692-826e-464f-83c1-77d6913e59b7",
   "metadata": {},
   "source": [
    "The only consitency with all the duplicate values in the `events` table is that they are all `offer completed` events. Therefore, I will leave them for now just incase another reason pops up when we join the tables later. <br>\n",
    "<br>\n",
    "Since the `events` table has has four distinct events. It seems benificial to break up the `events` table into the four distinct events and clean the now seperate tables accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47062c3-2684-4091-abc0-bbbb33c432d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the events table into each event type\n",
    "def divide_table(event):\n",
    "    filtered_table = events[events['event'] == event]\n",
    "    filtered_table = filtered_table.drop('event', axis=1)\n",
    "    if event == 'offer completed':\n",
    "        filtered_table = filtered_table.drop('value_key', axis=1).rename({'value': 'offer_id'}, axis=1)\n",
    "    elif event == 'transaction':\n",
    "        filtered_table = filtered_table.drop(['reward', 'value_key'], axis=1).rename({'value': 'amount'}, axis=1)\n",
    "    else:\n",
    "        filtered_table = filtered_table.drop(['reward', 'value_key'], axis=1).rename({'value': 'offer_id'}, axis=1)\n",
    "    return filtered_table\n",
    "\n",
    "offers_completed = divide_table('offer completed')\n",
    "offers_received = divide_table('offer received')\n",
    "offers_viewed = divide_table('offer viewed')\n",
    "transactions = divide_table('transaction')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfed25ef-2c26-420c-bbd6-84913ae2b14b",
   "metadata": {},
   "source": [
    "## Deporting Data to MySQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afc42bf-6392-4e99-b840-ffa1967a2b91",
   "metadata": {},
   "source": [
    "**Fix The Following Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2ee8e2-376d-4711-8b01-b97f7a1b707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'root'\n",
    "password = ''\n",
    "host = 'localhost'\n",
    "port = '3306'\n",
    "database = 'starbucks_db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1488e8f-bb58-4f31-b997-f8fe08b119a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#engine = create_engine(f'mysql+mysqlconnector://{user}:{password}@{host}/{database}')\n",
    "\n",
    "# Send the DataFrame to MySQL\n",
    "#offers.to_sql('my_table', con=engine, if_exists='replace', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
